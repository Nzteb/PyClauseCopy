# ************************************************************************
# *********** DEFAULT CONFIGURATION: DO NOT CHANGE THIS FILE *************
# ************************************************************************
 

## BACKEND c_clause handler options #######################################

# Example:
# from c_clause import DataHandler, QAHandler
# from clause.config.options import Options
# opts = Options()
# opts.set("data_handler.load_zero_rules", False)
# opts.set("data_handler.b_min_support", 10)
# loader = DataHandler(opts.flatS("data_handler"))
# qa = QAHandler(opts.flatS("qa_handler"))


###  DataHandler loads rulesets and datasets
###  the options can be used to filter/ignore some rules

# 'r_num_unseen' option:
# laplace smoothing applied to rule type 'r' for calculating confidence
# conf = support / (num_unseen+num_predictions)

# 'r_min_support' option:
#  dont load rules of type 'r' if they have a lower support as 'r_min_support'
# 'r_min_preds' option: as above for the overall number of triple predicitions
# 'r_min_confidence': as above; note: confidence here is without laplace smooting
data_handler:
  ## Z rules
  load_zero_rules: True
  # weight that is multiplied with the confidence
  z_weight: 0.1
  z_num_unseen: 5
  z_min_support: 2
  z_min_preds: 2
  z_min_conf: 0.0001

  ## U_c rules
  load_u_c_rules: True
  c_num_unseen: 5
  c_min_support: 2
  c_min_preds: 2
  c_min_conf: 0.0001

  ## B rules
  load_b_rules: True
  b_max_branching_factor: -1 #-1 for off
  b_num_unseen: 5
  b_min_support: 2
  b_min_preds: 2
  b_min_conf: 0.0001

  ##  U_d rules
  load_u_d_rules: True
  # weight that is multiplied with the confidence
  d_weight: 0.01
  d_max_branching_factor: -1 #-1 for off
  d_num_unseen: 5
  d_min_support: 2
  d_min_preds: 2
  d_min_conf: 0.0001

  ## U_xxc rules
  load_u_xxc_rules: True
  xxc_num_unseen: 5
  xxc_min_support: 2
  xxc_min_preds: 2
  xxc_min_conf: 0.0001

  ## U_xxd rules
  load_u_xxd_rules: True
  xxd_num_unseen: 5
  xxd_min_support: 2
  xxd_min_preds: 2
  xxd_min_conf: 0.0001

# calculates rankings based on the target set loaded into DataLoader
ranking_handler:
  # whether to cache the rules that predicted the query candidates,
  # can be retrieved with handler.get_rules(bool, string:headOrTailRanking)
  # turn off for efficiency.
  collect_rules: False
  # number of candidates to calculate
  topk: 100
  # select from "maxplus" / "noisyor"
  # maxplus scores of a ranking will be of the highest predicting rule
  # candidate discrimination is based on comparing the sequences of predicting
  # rules confidences lexicographically
  # noisyor scores and ranking is based on sorting the noisy-or product
  # the noisy-or sorting is based on -\sum_i(log(1-conf_i)) and transformed
  # berfore outputted; this mitigates floating point considerations 
  aggregation_function: "maxplus"

  ### stopping criteria for rule application

  # stop rule application for a query if topk candidates are calculated AND at least disc_at_least
  # of the best candidates are fully discriminated, i.e, they are pairwise distinct in regard
  # to their predicting rules
  # recommended values: 10 or 20 under topk=100
  disc_at_least: 10 # -1 for off, must not be bigger than topk

  # stop rule application for a query as soon as hard_stop_at candidates
  # are found (ignoring topk); set the value to topk under maxplus to achieve max-aggregation
  # scores for all candidates very fast without getting a properly discriminated ranking
  # recommended value: -1
  hard_stop_at: -1 #-1 for off

  # stops adding predicting rules to a candidate of a query if already num_top_rules
  # predicted the candidate; if all candidates are predicted by num_top_rules, rule
  # application is stopped; can be used in conjunction with "noisyor" to achieve
  # noisy-or top-h (https://arxiv.org/pdf/2309.00306.pdf)
  # recommended values: -1 under "maxplus"; 5 under "noisyor"
  num_top_rules: -1

  filter_w_train: True
  filter_w_target: True
  # choose between "random" / "frequency"
  tie_handling: "frequency"
  # -1 for using ALL available threads
  num_threads: -1 

# calculates answer candidates and scores based on 
# questions (h, r, ?) and (?, r, t)
qa_handler:
  # same as ranking_handler; can be retrieved with handler.get_rules(bool)
  # see documentation for data strucuture
  collect_rules: False
    # number of candidates to calculate
  topk: 100
  # select from "maxplus" / "noisyor"; see ranking handler
  aggregation_function: "maxplus"

  ### stopping criteria for rule application
  # see ranking_handler for description
  disc_at_least: 10 # -1 for off, must not be bigger than topk
  hard_stop_at: -1 #-1 for off
  num_top_rules: -1

  filter_w_train: True
  # choose between "random" / "frequency"
  tie_handling: "frequency"
  # -1 for using ALL available threads
  num_threads: -1 

# given input rules, calculates materialization (predictions)
# and stats (num_pred, num_true_preds)
rules_handler:
  # whether to store triple predictions 
  # and stats (num_pred, num_true)
  # to obtained with handler.get_predictions(bool), handler.get_statistics()
  collect_predictions: True
  collect_statistics: True
  num_threads: -1 #-1 for ALL available threads

# given input triples, calculates triple scores and can output all
# predicting rules + their groundings (explanations)
prediction_handler:
  collect_explanations: False
  # select from "maxplus" / "noisyor"
  # note that as we are not calculating candidate rankings; selecting maxplus simply
  # results in "max-aggregation" scores
  aggregation_function: "maxplus"
  # for a given triple stop rule application if it was predicted by the
  # num_top_rules with the highest confidences
  # set to -1 to not apply any stopping criterion, e.g., to apply all rules
  # if you want to only collect the best explanation for each triple, set to 1
  # note that the noisyor score is influenced by this parameter
  # and results in noisyor-top-h scores (https://arxiv.org/pdf/2309.00306.pdf)
  num_top_rules: 5 
  
  num_threads: -1 #-1 for ALL available threads


## FRONTEND clause options ###############################################

io:
  rule_format: "PyClause"
learning:
  # chose betweeen:
  # "anyburl" / "hybrid" (mixing anyburl any nanytorm) / "nanytorm" / "amie"
  mode: "amie"  
  anyburl:
    time: 60
    # you can set any raw options supported by AnyBURL
    raw:
      MAX_LENGTH_CYCLIC: 3
      THRESHOLD_CORRECT_PREDICTIONS: 2
      THRESHOLD_CONFIDENCE: 0.0001
  amie:
    raw: # the -symbol infront of each param is added automatically
      mins: 2
      maxad: 4 # number of atoms = 1 head atom + body atoms
      minhc: 0.0001
      minpca: 0.0001
  torm:
    # if set to false, rules that do not make any wrong prediction are surpressed
    tautology: False 
    b:
      active: True
      confidence: 0.0001
      support: 2
      length: 3
      batchsize: 1000
    uc:
      active: True
      confidence: 0.0001
      support: 2
    ud:
      active: True
      confidence: 0.0001
      support: 2
    z:
      active: True
      confidence: 0.0001
      support: 2
    xx_uc:
      active: True
      confidence: 0.0001
      support: 2
    xx_ud:
      active: True
      confidence: 0.0001
      support: 2